{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import string\n",
    "from itertools import islice\n",
    "\n",
    "def printNDict(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "def extract_content(path):\n",
    "    with open(path, 'r') as book:\n",
    "        content = book.read()\n",
    "    return content\n",
    "\n",
    "def remove_gutenberg_text(content):\n",
    "    paragraphs = (p for p in content.split('\\n') if p != '')\n",
    "    include = False\n",
    "    START_PREFIX = '***START OF'\n",
    "    END_PREFIX = '***END OF'\n",
    "    \n",
    "    non_gutenberg_paragraphs = []\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph[:len(END_PREFIX)] == END_PREFIX:\n",
    "            include = False\n",
    "        \n",
    "        if include:\n",
    "            non_gutenberg_paragraphs.append(paragraph)\n",
    "        \n",
    "        if paragraph[:len(START_PREFIX)] == START_PREFIX:\n",
    "            include = True\n",
    "    \n",
    "    return '\\n'.join(non_gutenberg_paragraphs)\n",
    "\n",
    "def extract_sentences(book):\n",
    "    return nltk.tokenize.sent_tokenize(book)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return nltk.tokenize.word_tokenize(sentence)\n",
    "\n",
    "def lookup_token(token, vocabulary):\n",
    "    result = vocabulary.get(token.lower())\n",
    "    if result is None:\n",
    "        result = vocabulary[OOV]\n",
    "    return result\n",
    "\n",
    "def lookup_index(index, indexed_vocabulary):\n",
    "    return indexed_vocabulary[index]\n",
    "\n",
    "def encode_document(document, vocabulary):\n",
    "    return [lookup_token(token, vocabulary) for token in document]\n",
    "\n",
    "def decode_document(encoded_document, indexed_vocabulary):\n",
    "    return [lookup_index(index, indexed_vocabulary) for index in encoded_document]\n",
    "\n",
    "def calculate_frequencies(encoded_document, vocabulary):\n",
    "    return np.bincount(encoded_document, minlength=len(vocabulary))\n",
    "\n",
    "\n",
    "\n",
    "english_words = nltk.corpus.words.words()\n",
    "\n",
    "lower_english_words = {word.lower() for word in english_words}\n",
    "\n",
    "punctuation_tokens = set(string.punctuation)\n",
    "\n",
    "OOV = '<oov>'\n",
    "non_word_tokens = punctuation_tokens.union({OOV})\n",
    "\n",
    "indexed_base_vocabulary = sorted(list(lower_english_words.union(non_word_tokens)))\n",
    "\n",
    "\n",
    "base_vocabulary = {indexed_base_vocabulary[i]:i for i in range(len(indexed_base_vocabulary))}\n",
    "\n",
    "n_items = printNDict(10, base_vocabulary.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<map object at 0x11dcbb908>\n"
     ]
    }
   ],
   "source": [
    "DARWIN_DIR = 'darwin'\n",
    "DICKENS_DIR = 'dickens'\n",
    "\n",
    "pd.read_csv(os.path.join(DARWIN_DIR, 'metadata.tsv'), delimiter='\\t')\n",
    "pd.read_csv(os.path.join(DICKENS_DIR, 'metadata.tsv'), delimiter='\\t')\n",
    "\n",
    "def build_corpora(vocabulary):\n",
    "    directories = [('darwin', DARWIN_DIR), ('dickens', DICKENS_DIR)]\n",
    "    \n",
    "    book_paths = list(map(\n",
    "        lambda p: (p[0], glob.glob(os.path.join(p[1], '*.txt'))),\n",
    "        directories\n",
    "    ))\n",
    "    \n",
    "    books = list(map(\n",
    "        lambda p: (\n",
    "            p[0],\n",
    "            (remove_gutenberg_text(extract_content(path)) for path in p[1])\n",
    "        ),\n",
    "        book_paths\n",
    "    ))\n",
    "    \n",
    "    sentences = map(\n",
    "        lambda p: (\n",
    "            p[0],\n",
    "            (sentence for book in p[1] for sentence in extract_sentences(book))\n",
    "        ),\n",
    "        books\n",
    "    )\n",
    "    \n",
    "    documents = map(\n",
    "        lambda p: (\n",
    "            p[0],\n",
    "            (tokenize(sentence) for sentence in p[1])\n",
    "        ),\n",
    "        sentences\n",
    "    )\n",
    "    \n",
    "    encoded_sentences = list(map(\n",
    "        lambda p: (\n",
    "            p[0],\n",
    "            [encode_document(document, vocabulary) for document in p[1]]\n",
    "        ),\n",
    "        documents\n",
    "    ))\n",
    "    \n",
    "    return dict(encoded_sentences)\n",
    "\n",
    "directories = [('darwin', DARWIN_DIR), ('dickens', DICKENS_DIR)]\n",
    "\n",
    "book_paths = list(map(\n",
    "    lambda p: (p[0], glob.glob(os.path.join(p[1], '*.txt'))),\n",
    "    directories\n",
    "))\n",
    "\n",
    "books = list(map(\n",
    "        lambda p: (\n",
    "            p[0],\n",
    "            (remove_gutenberg_text(extract_content(path)) for path in p[1])\n",
    "        ),\n",
    "        book_paths\n",
    "    ))\n",
    "\n",
    "sentences = map(\n",
    "    lambda p: (\n",
    "        p[0],\n",
    "        (sentence for book in p[1] for sentence in extract_sentences(book))\n",
    "    ),\n",
    "    books\n",
    ")\n",
    "\n",
    "print(sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
